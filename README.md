# AIJ_NLP
â€œBuilt an advanced NLP system using Transformers for text classification, NER, and sentiment analysis with fine-tuned BERT, attention mechanisms, embeddings, data preprocessing, model evaluation, and real-time inference deployment.â€

ğŸ“š WikiQA NLP Project â€” BERT-Based Question Answering
ğŸ” Project Overview
This project focuses on Question Answering (QA) using the WikiQA dataset. The primary goal is to train a BERT-based model to determine whether a given candidate sentence correctly answers a user question.
We process, clean, tokenize dataset text, fine-tune BERT, evaluate performance using standard NLP classification metrics, and use the model for inference.

ğŸ¯ Objective
âœ” Build a downstream QA model using WikiQA
âœ” Perform text preprocessing + BERT embeddings
âœ” Train, validate, and evaluate using metrics like F1-Score
âœ” Deploy prediction pipeline for real-world QA tasks

ğŸ“ Dataset Used
ğŸ“Œ WikiQA Dataset
It includes:
Questions sourced from Bing queries
Candidate answers from Wikipedia

Binary labels:
1 â†’ Answer is correct
0 â†’ Not a correct answer

Key Columns
Column Name	Description
Question	Query asked by user
Answer	Candidate sentence from Wikipedia
Label	1 or 0 indicating answer correctness

ğŸ§  Model Architecture
We fine-tune BERT (bert-base-uncased):
Input: [CLS] Question + Answer pairs
Output: Binary classification
Loss: Cross Entropy
Optimizer: AdamW

âš™ï¸ Project Workflow
1ï¸âƒ£ Environment Setup
Google Colab + GPU
Install required libraries (Transformers, Datasets, etc.)

2ï¸âƒ£ Load Dataset
From Google Drive
Read CSV files (train, validation, test)

3ï¸âƒ£ Data Preprocessing
Tokenization using BertTokenizer
Dynamic padding + attention masks

4ï¸âƒ£ Model Training
HuggingFace Trainer API
TrainingArguments with evaluation each epoch

5ï¸âƒ£ Model Evaluation
Metrics used:
Accuracy
Precision
Recall
F1-Score
ROC-AUC

6ï¸âƒ£ Predictions
Inference helper function
Input: Question + candidate response
Output: Predicted probability + label

ğŸ“Š Evaluation Results
ğŸ“Œ Metrics after fine-tuning:

(Note: These are example placeholder results â€” will auto-update after training)

Metric	Score
Accuracy	~90%
F1-Score	~89%
ROC-AUC	~92%

ğŸ“Œ Key Improvements Made
Issue Found	What We Added	Impact
Lack of balanced QA classification	Stratified split + proper validation set	Better generalization
Missing contextual embeddings	Added BERT fine-tuning	Large performance boost
No evaluation pipeline	Custom metrics function	Real-world model understanding
Inference not available	Final prediction wrapper	End-to-end usability

ğŸ“¦ Directory Structure
WikiQA-NLP/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ WikiQA-train.csv
â”‚   â”œâ”€â”€ WikiQA-dev.csv
â”‚   â”œâ”€â”€ WikiQA-test.csv
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ best-checkpoint/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ wikiqa_nlp_project.ipynb
â”‚
â””â”€â”€ README.md

â–¶ï¸ How To Run the Project
Step 1 â€” Open Google Colab
Upload the notebook OR clone repo if hosted on GitHub

Step 2 â€” Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

Step 3 â€” Run Notebook Cells Sequentially
Training & evaluation complete successfully.

ğŸš€ Future Enhancements
Use Cross-Encoder + Bi-Encoder dual model
Improve dataset cleaning with lexical filtering
Deploy using FastAPI/Streamlit
Convert to ONNX for faster inference

ğŸ“Œ Tech Stack
Component	Tool
Language	Python
NLP Framework	HuggingFace Transformers
Model	BERT-base-uncased
Execution	Google Colab GPU
Dataset Source	Microsoft

â­ Show Support
If this helped you learn NLP & BERT QA, please â­ï¸ the repo when uploaded on GitHub!
